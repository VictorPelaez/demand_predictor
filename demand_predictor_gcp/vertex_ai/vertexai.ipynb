{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d3001-80fa-4e25-974d-adcbf7f5d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\"\n",
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.4.3 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.6 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4068458a-829c-4ab5-9591-c1b775bbae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vpproyecto2021\n",
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n",
      "gs://vpproyecto2021/models\n",
      "gs://vpproyecto2021/data/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"myproyecto2021\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "BUCKET_NAME=\"gs://\" + PROJECT_ID  \n",
    "\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}\"\n",
    "\n",
    "DATA_ROOT='{}/data'.format(BUCKET_NAME)\n",
    "TRAINING_FILE_PATH='{}/{}'.format(DATA_ROOT, 'dataset.csv')\n",
    "OUTPUT_MODEL = '{}/models'.format(BUCKET_NAME)\n",
    "print(OUTPUT_MODEL)\n",
    "print(TRAINING_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6362d1f-a4ee-4caf-969d-db2485ca52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea670bac-2c85-4fd1-86e6-c570d4e35f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Train component\n",
    "##############\n",
    "@component(\n",
    "    base_image='python:3.9', # Use a different base image.\n",
    "    packages_to_install=['tensorflow']\n",
    ")\n",
    "\n",
    "def train(training_dataset_path: str,\n",
    "          # output_model: Output[Model],\n",
    "          window_size: int,\n",
    "          batch_size: int,\n",
    "          epochs: int,\n",
    "          lr: float):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: train script\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    LR = lr\n",
    "    \n",
    "    l0 = tf.keras.layers.Dense(2*window_size+1, input_shape=[window_size], activation='relu')\n",
    "    l2 = tf.keras.layers.Dense(1)\n",
    "    model = tf.keras.models.Sequential([l0, l2])\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3)\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=LR, momentum=0.9)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    # load data\n",
    "    data = pd.read_csv(training_dataset_path)\n",
    "    time = np.array(data.times)\n",
    "    series = np.array(data.values)[:,1].astype('float32')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1])) \n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    trainds = dataset\n",
    "    \n",
    "    # (trainds,series) = load_dataset(pattern=training_dataset_path, window_size=window_size, batch_size=batch_size)\n",
    "    # evalds = load_dataset(pattern=validation_dataset_path, mode='eval')\n",
    "    \n",
    "    history = model.fit(trainds, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    # Save model in Metadata\n",
    "    # model.save(output_model.path)\n",
    "    # logging.info('using model.uri: %s', output_model.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b5b2319-5f08-4aa9-8662-d1e7225294bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name='spanish-demand-forecasting',\n",
    "                  description='Pipeline to create training')\n",
    "\n",
    "def pipeline(message: str):\n",
    "    train_task = train(\n",
    "        training_dataset_path=TRAINING_FILE_PATH,\n",
    "        # output_model=OUTPUT_MODEL,\n",
    "        window_size=30,\n",
    "        batch_size=16,\n",
    "        epochs=10,\n",
    "        lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78004971-dcc9-47d2-8b06-498d4fa99b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"demand_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9c8aa4d-e450-416a-a5ff-ad7b4b91b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline_job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"demand_pipeline\",\n",
    "    template_path=\"demand_pipeline.json\",\n",
    "    # job_id=\"pipeline-lwpython-tf-uscentral1-{0}\".format(TIMESTAMP),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={'message': \"Hello, World\"},\n",
    "    enable_caching=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3664949-9294-48c8-8feb-f2838387b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/508206629205/locations/us-central1/pipelineJobs/spanish-demand-forecasting-20210924132908\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/508206629205/locations/us-central1/pipelineJobs/spanish-demand-forecasting-20210924132908')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/spanish-demand-forecasting-20210924132908?project=508206629205\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/508206629205/locations/us-central1/pipelineJobs/spanish-demand-forecasting-20210924132908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/508206629205/locations/us-central1/pipelineJobs/spanish-demand-forecasting-20210924132908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/508206629205/locations/us-central1/pipelineJobs/spanish-demand-forecasting-20210924132908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/508206629205/locations/us-central1/pipelineJobs/spanish-demand-forecasting-20210924132908 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [train].; Job (project_id = vpproyecto2021, job_id = 3602255179291820032) is failed due to the above error.; Failed to handle the job: {project_number = 508206629205, job_id = 3602255179291820032}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22541/1660283535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mml_pipeline_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Pipeline Job:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [train].; Job (project_id = vpproyecto2021, job_id = 3602255179291820032) is failed due to the above error.; Failed to handle the job: {project_number = 508206629205, job_id = 3602255179291820032}\"\n"
     ]
    }
   ],
   "source": [
    "ml_pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766af6fa-1583-499a-b853-ba98b12caf68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
