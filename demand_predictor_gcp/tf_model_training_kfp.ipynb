{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988f5b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-35d32fccdc52\n",
      "gs://qwiklabs-gcp-00-35d32fccdc52/data/dataset.csv\n",
      "gs://qwiklabs-gcp-00-35d32fccdc52/models\n"
     ]
    }
   ],
   "source": [
    "# set variables\n",
    "# PROJECT_ID = \"qwiklabs-gcp-00-35d32fccdc52\"\n",
    "\n",
    "REGION = 'us-central1'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BUCKET = 'gs://' + PROJECT_ID\n",
    "print(BUCKET)\n",
    "\n",
    "DATA_ROOT='{}/data'.format(BUCKET)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(BUCKET)\n",
    "TRAINING_FILE_PATH='{}/{}'.format(DATA_ROOT, 'dataset.csv')\n",
    "# VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')\n",
    "OUTPUT_DIR = '{}/models'.format(BUCKET)\n",
    "print(TRAINING_FILE_PATH)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb2067",
   "metadata": {},
   "source": [
    "# TF in a KFP running in AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780dd3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tensorflow_trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/train.py\n",
    "\n",
    "\"\"\"Tensorflow predictor script.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import fire\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(pattern, window_size=30, batch_size=16, shuffle_buffer=100):\n",
    "    \"\"\"\n",
    "    Description:  \n",
    "    Input: \n",
    "      - series:\n",
    "      - window_size:\n",
    "      - batch_size: the batches to use when training\n",
    "      -shuffle_buffer: size buffer, how data will be shuffled\n",
    "\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # read data\n",
    "    data = pd.read_csv(pattern)\n",
    "    time = np.array(data.times)\n",
    "    series = np.array(data.values)[:,1].astype('float32')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1])) # x and y (last one)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset, series\n",
    "\n",
    "def train_evaluate(training_dataset_path, \n",
    "                   # validation_dataset_path,\n",
    "                   window_size,\n",
    "                   batch_size,\n",
    "                   epochs,\n",
    "                   lr,\n",
    "                   # num_train_examples, num_evals, \n",
    "                   output_dir):\n",
    "    \"\"\"\n",
    "    Description: train script\n",
    "    \"\"\"\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    LR = lr\n",
    "    \n",
    "    l0 = tf.keras.layers.Dense(2*window_size+1, input_shape=[window_size], activation='relu')\n",
    "    l2 = tf.keras.layers.Dense(1)\n",
    "    model = tf.keras.models.Sequential([l0, l2])\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3)\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=LR, momentum=0.9)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    # load data\n",
    "    (trainds,series) = load_dataset(pattern=training_dataset_path, window_size=window_size, batch_size=batch_size)\n",
    "    # evalds = load_dataset(pattern=validation_dataset_path, mode='eval')\n",
    "    \n",
    "    history = model.fit(trainds, epochs=EPOCHS, verbose=0)\n",
    "       \n",
    "    if hptune:\n",
    "        # trainds = load_dataset(pattern=training_dataset_path, window_size=window_size, batch_size=batch_size)\n",
    "        results = np.array(model.predict(trainds))[:, 0]\n",
    "        mae = tf.keras.metrics.mean_absolute_error(series[window_size:], results).numpy()  \n",
    "        print('Model accuracy: {}'.format(mae))# Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='mean_absolute_error', metric_value=mae)\n",
    "               \n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        EXPORT_PATH = os.path.join(output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        tf.saved_model.save(obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "        \n",
    "        # model_filename = 'model.pkl'\n",
    "        # with open(model_filename, 'wb') as model_file:\n",
    "            # pickle.dump(pipeline, model_file)\n",
    "            # gcs_model_path = '{}/{}'.format(job_dir, model_filename)\n",
    "            # subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        #print('Saved model in: {}'.format(gcs_model_path))\n",
    "        \n",
    "        print(\"Exported trained model to {}\".format(EXPORT_PATH))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c4cc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tensorflow_trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire tensorflow==2.1.1\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e19ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IMAGE_NAME='tensorflow_trainer_image'\n",
    "TF_IMAGE_TAG='latest'\n",
    "TF_IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, TF_IMAGE_NAME, TF_IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d61432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 3.4 KiB before compression.\n",
      "Uploading tarball of [tensorflow_trainer_image] to [gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516268.184746-2e1134b6934044be8f8d83576b3bc6e3.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-35d32fccdc52/locations/global/builds/c1a72656-1516-4c67-bd40-eff179fb02db].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c1a72656-1516-4c67-bd40-eff179fb02db?project=1050598426634].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c1a72656-1516-4c67-bd40-eff179fb02db\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516268.184746-2e1134b6934044be8f8d83576b3bc6e3.tgz#1631516269232037\n",
      "Copying gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516268.184746-2e1134b6934044be8f8d83576b3bc6e3.tgz#1631516269232037...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "04d61753a965: Pulling fs layer\n",
      "4c841a8dc1c2: Pulling fs layer\n",
      "ba19310bc52a: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "187fc16bda8d: Pulling fs layer\n",
      "be760be104b7: Pulling fs layer\n",
      "67ef79a8e023: Pulling fs layer\n",
      "31110891cfb1: Pulling fs layer\n",
      "8c8c23a31282: Pulling fs layer\n",
      "b1bb999ddb16: Pulling fs layer\n",
      "3ce4958fd428: Pulling fs layer\n",
      "77615062896e: Pulling fs layer\n",
      "a3ad053dcd2a: Pulling fs layer\n",
      "60b86c84dfcb: Pulling fs layer\n",
      "bb2debff4a41: Pulling fs layer\n",
      "502011482b4c: Pulling fs layer\n",
      "26c66a4831b0: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "187fc16bda8d: Waiting\n",
      "be760be104b7: Waiting\n",
      "67ef79a8e023: Waiting\n",
      "31110891cfb1: Waiting\n",
      "8c8c23a31282: Waiting\n",
      "b1bb999ddb16: Waiting\n",
      "3ce4958fd428: Waiting\n",
      "ba19310bc52a: Waiting\n",
      "bb2debff4a41: Waiting\n",
      "77615062896e: Waiting\n",
      "502011482b4c: Waiting\n",
      "a3ad053dcd2a: Waiting\n",
      "26c66a4831b0: Waiting\n",
      "60b86c84dfcb: Waiting\n",
      "04d61753a965: Download complete\n",
      "e4ca327ec0e7: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "187fc16bda8d: Verifying Checksum\n",
      "187fc16bda8d: Download complete\n",
      "ba19310bc52a: Verifying Checksum\n",
      "ba19310bc52a: Download complete\n",
      "67ef79a8e023: Verifying Checksum\n",
      "67ef79a8e023: Download complete\n",
      "31110891cfb1: Verifying Checksum\n",
      "31110891cfb1: Download complete\n",
      "8c8c23a31282: Verifying Checksum\n",
      "8c8c23a31282: Download complete\n",
      "b1bb999ddb16: Download complete\n",
      "3ce4958fd428: Verifying Checksum\n",
      "3ce4958fd428: Download complete\n",
      "77615062896e: Download complete\n",
      "a3ad053dcd2a: Verifying Checksum\n",
      "a3ad053dcd2a: Download complete\n",
      "60b86c84dfcb: Verifying Checksum\n",
      "60b86c84dfcb: Download complete\n",
      "bb2debff4a41: Verifying Checksum\n",
      "bb2debff4a41: Download complete\n",
      "be760be104b7: Verifying Checksum\n",
      "be760be104b7: Download complete\n",
      "26c66a4831b0: Verifying Checksum\n",
      "26c66a4831b0: Download complete\n",
      "4c841a8dc1c2: Verifying Checksum\n",
      "4c841a8dc1c2: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "04d61753a965: Pull complete\n",
      "502011482b4c: Verifying Checksum\n",
      "502011482b4c: Download complete\n",
      "4c841a8dc1c2: Pull complete\n",
      "ba19310bc52a: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "187fc16bda8d: Pull complete\n",
      "be760be104b7: Pull complete\n",
      "67ef79a8e023: Pull complete\n",
      "31110891cfb1: Pull complete\n",
      "8c8c23a31282: Pull complete\n",
      "b1bb999ddb16: Pull complete\n",
      "3ce4958fd428: Pull complete\n",
      "77615062896e: Pull complete\n",
      "a3ad053dcd2a: Pull complete\n",
      "60b86c84dfcb: Pull complete\n",
      "bb2debff4a41: Pull complete\n",
      "502011482b4c: Pull complete\n",
      "26c66a4831b0: Pull complete\n",
      "Digest: sha256:d13107221c97d56ffa891a44beaecaa8438c87102ec7e399f97c77d70c7cd7e4\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> b99046b5f2b5\n",
      "Step 2/5 : RUN pip install -U fire tensorflow==2.1.1\n",
      " ---> Running in e6c46de5af09\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting tensorflow==2.1.1\n",
      "  Downloading tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (0.37.0)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (3.16.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.38.1)\n",
      "Collecting keras-preprocessing==1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.12.1)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (57.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.4.6)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.1.1)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.5.0)\n",
      "Building wheels for collected packages: gast, fire, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=7301174f7a267e6fd5dbced7397dfb4a39c9d4c94335052b055a50cb8b9362e8\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=81cb7b887d4197931a5e8c2810e0908ab384ab378d443b12d2f1d1d47ec86067\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=96f7cd474d4031be0a66d506c65646cc5661cd14cc1ea57af297d6e638e61bd3\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built gast fire termcolor\n",
      "Installing collected packages: cached-property, werkzeug, h5py, absl-py, termcolor, tensorflow-estimator, tensorboard, scipy, opt-einsum, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow, fire\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.1\n",
      "    Uninstalling scipy-1.7.1:\n",
      "      Successfully uninstalled scipy-1.7.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "phik 0.11.2 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "\u001b[0mSuccessfully installed absl-py-0.13.0 astor-0.8.1 cached-property-1.5.2 fire-0.4.0 gast-0.2.2 google-pasta-0.2.0 h5py-3.4.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 opt-einsum-3.3.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.1 tensorflow-estimator-2.1.0 termcolor-1.1.0 werkzeug-2.0.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e6c46de5af09\n",
      " ---> 38c14e1becf6\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 6018758ca09b\n",
      "Removing intermediate container 6018758ca09b\n",
      " ---> 927333ba645e\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> d292e7c8f652\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 5c307ce15bdb\n",
      "Removing intermediate container 5c307ce15bdb\n",
      " ---> 102ac38077fc\n",
      "Successfully built 102ac38077fc\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-00-35d32fccdc52/tensorflow_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-00-35d32fccdc52/tensorflow_trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-35d32fccdc52/tensorflow_trainer_image]\n",
      "99a2cbbabede: Preparing\n",
      "77ef0c13d07d: Preparing\n",
      "a28940c7876c: Preparing\n",
      "a34559070c4d: Preparing\n",
      "5b1ab540351c: Preparing\n",
      "7e351b6f48b7: Preparing\n",
      "3e4234304903: Preparing\n",
      "fdc0888feca9: Preparing\n",
      "aeae925dc9db: Preparing\n",
      "1bd7873b0779: Preparing\n",
      "376d2b415584: Preparing\n",
      "44f70d274c58: Preparing\n",
      "e7d923b0bae8: Preparing\n",
      "7e2741fe6f4c: Preparing\n",
      "18fbfa48ed76: Preparing\n",
      "51943bd297fe: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "c4594481f563: Preparing\n",
      "97982642ef8a: Preparing\n",
      "251dde02ee49: Preparing\n",
      "6babb56be259: Preparing\n",
      "7e351b6f48b7: Waiting\n",
      "3e4234304903: Waiting\n",
      "fdc0888feca9: Waiting\n",
      "aeae925dc9db: Waiting\n",
      "1bd7873b0779: Waiting\n",
      "376d2b415584: Waiting\n",
      "44f70d274c58: Waiting\n",
      "e7d923b0bae8: Waiting\n",
      "7e2741fe6f4c: Waiting\n",
      "18fbfa48ed76: Waiting\n",
      "51943bd297fe: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "c4594481f563: Waiting\n",
      "97982642ef8a: Waiting\n",
      "251dde02ee49: Waiting\n",
      "6babb56be259: Waiting\n",
      "5b1ab540351c: Mounted from deeplearning-platform-release/base-cpu\n",
      "a34559070c4d: Mounted from deeplearning-platform-release/base-cpu\n",
      "7e351b6f48b7: Mounted from deeplearning-platform-release/base-cpu\n",
      "3e4234304903: Mounted from deeplearning-platform-release/base-cpu\n",
      "77ef0c13d07d: Pushed\n",
      "99a2cbbabede: Pushed\n",
      "fdc0888feca9: Mounted from deeplearning-platform-release/base-cpu\n",
      "aeae925dc9db: Mounted from deeplearning-platform-release/base-cpu\n",
      "376d2b415584: Mounted from deeplearning-platform-release/base-cpu\n",
      "1bd7873b0779: Mounted from deeplearning-platform-release/base-cpu\n",
      "44f70d274c58: Mounted from deeplearning-platform-release/base-cpu\n",
      "e7d923b0bae8: Mounted from deeplearning-platform-release/base-cpu\n",
      "7e2741fe6f4c: Mounted from deeplearning-platform-release/base-cpu\n",
      "5f70bf18a086: Layer already exists\n",
      "18fbfa48ed76: Mounted from deeplearning-platform-release/base-cpu\n",
      "51943bd297fe: Mounted from deeplearning-platform-release/base-cpu\n",
      "c4594481f563: Mounted from deeplearning-platform-release/base-cpu\n",
      "97982642ef8a: Mounted from deeplearning-platform-release/base-cpu\n",
      "6babb56be259: Layer already exists\n",
      "251dde02ee49: Mounted from deeplearning-platform-release/base-cpu\n",
      "a28940c7876c: Pushed\n",
      "latest: digest: sha256:6221ef6c6ffc43cc88f2ef13f311750d6a710eb54a931be79f377d0dffaf73e9 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                  STATUS\n",
      "c1a72656-1516-4c67-bd40-eff179fb02db  2021-09-13T06:57:49+00:00  5M59S     gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516268.184746-2e1134b6934044be8f8d83576b3bc6e3.tgz  gcr.io/qwiklabs-gcp-00-35d32fccdc52/tensorflow_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TF_IMAGE_URI $TF_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63714fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2a5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire tensorflow==2.1.1 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936eb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b7156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 120 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516658.012807-2892ce773dfb4738a23bfda1f37c4703.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-35d32fccdc52/locations/global/builds/823c074a-c462-4331-8c1d-9a9debbc6097].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/823c074a-c462-4331-8c1d-9a9debbc6097?project=1050598426634].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"823c074a-c462-4331-8c1d-9a9debbc6097\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516658.012807-2892ce773dfb4738a23bfda1f37c4703.tgz#1631516658341445\n",
      "Copying gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516658.012807-2892ce773dfb4738a23bfda1f37c4703.tgz#1631516658341445...\n",
      "/ [1 files][  230.0 B/  230.0 B]                                                \n",
      "Operation completed over 1 objects/230.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "04d61753a965: Pulling fs layer\n",
      "4c841a8dc1c2: Pulling fs layer\n",
      "ba19310bc52a: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ba19310bc52a: Waiting\n",
      "187fc16bda8d: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "be760be104b7: Pulling fs layer\n",
      "67ef79a8e023: Pulling fs layer\n",
      "31110891cfb1: Pulling fs layer\n",
      "187fc16bda8d: Waiting\n",
      "be760be104b7: Waiting\n",
      "67ef79a8e023: Waiting\n",
      "8c8c23a31282: Pulling fs layer\n",
      "b1bb999ddb16: Pulling fs layer\n",
      "31110891cfb1: Waiting\n",
      "8c8c23a31282: Waiting\n",
      "3ce4958fd428: Pulling fs layer\n",
      "77615062896e: Pulling fs layer\n",
      "b1bb999ddb16: Waiting\n",
      "a3ad053dcd2a: Pulling fs layer\n",
      "3ce4958fd428: Waiting\n",
      "77615062896e: Waiting\n",
      "60b86c84dfcb: Pulling fs layer\n",
      "bb2debff4a41: Pulling fs layer\n",
      "502011482b4c: Pulling fs layer\n",
      "26c66a4831b0: Pulling fs layer\n",
      "a3ad053dcd2a: Waiting\n",
      "60b86c84dfcb: Waiting\n",
      "bb2debff4a41: Waiting\n",
      "502011482b4c: Waiting\n",
      "26c66a4831b0: Waiting\n",
      "04d61753a965: Download complete\n",
      "e4ca327ec0e7: Verifying Checksum\n",
      "e4ca327ec0e7: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "187fc16bda8d: Verifying Checksum\n",
      "187fc16bda8d: Download complete\n",
      "ba19310bc52a: Verifying Checksum\n",
      "ba19310bc52a: Download complete\n",
      "67ef79a8e023: Verifying Checksum\n",
      "67ef79a8e023: Download complete\n",
      "31110891cfb1: Verifying Checksum\n",
      "31110891cfb1: Download complete\n",
      "8c8c23a31282: Verifying Checksum\n",
      "8c8c23a31282: Download complete\n",
      "b1bb999ddb16: Verifying Checksum\n",
      "b1bb999ddb16: Download complete\n",
      "3ce4958fd428: Verifying Checksum\n",
      "3ce4958fd428: Download complete\n",
      "77615062896e: Verifying Checksum\n",
      "77615062896e: Download complete\n",
      "a3ad053dcd2a: Verifying Checksum\n",
      "a3ad053dcd2a: Download complete\n",
      "60b86c84dfcb: Verifying Checksum\n",
      "60b86c84dfcb: Download complete\n",
      "bb2debff4a41: Verifying Checksum\n",
      "bb2debff4a41: Download complete\n",
      "be760be104b7: Verifying Checksum\n",
      "be760be104b7: Download complete\n",
      "26c66a4831b0: Verifying Checksum\n",
      "26c66a4831b0: Download complete\n",
      "4c841a8dc1c2: Verifying Checksum\n",
      "4c841a8dc1c2: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "04d61753a965: Pull complete\n",
      "502011482b4c: Verifying Checksum\n",
      "502011482b4c: Download complete\n",
      "4c841a8dc1c2: Pull complete\n",
      "ba19310bc52a: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "187fc16bda8d: Pull complete\n",
      "be760be104b7: Pull complete\n",
      "67ef79a8e023: Pull complete\n",
      "31110891cfb1: Pull complete\n",
      "8c8c23a31282: Pull complete\n",
      "b1bb999ddb16: Pull complete\n",
      "3ce4958fd428: Pull complete\n",
      "77615062896e: Pull complete\n",
      "a3ad053dcd2a: Pull complete\n",
      "60b86c84dfcb: Pull complete\n",
      "bb2debff4a41: Pull complete\n",
      "502011482b4c: Pull complete\n",
      "26c66a4831b0: Pull complete\n",
      "Digest: sha256:d13107221c97d56ffa891a44beaecaa8438c87102ec7e399f97c77d70c7cd7e4\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> b99046b5f2b5\n",
      "Step 2/2 : RUN pip install -U fire tensorflow==2.1.1 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in a55cdf32b844\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting tensorflow==2.1.1\n",
      "  Downloading tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.38.1)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (3.16.0)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting keras-preprocessing==1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.19.5)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (0.37.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.12.1)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2021.5.30)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.42.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.1.0)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.4.7)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.35.0)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (57.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.2.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.0.0)\n",
      "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.31.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.25.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (21.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.53.0)\n",
      "Requirement already satisfied: google-crc32c<=1.1.2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.1.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.8.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (21.2.0)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<3.0dev,>=1.29.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (4.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.4.6)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.1.1)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==0.2.5) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==0.2.5) (3.5.0)\n",
      "Building wheels for collected packages: kfp, argo-models, gast, tabulate, fire, kfp-server-api, termcolor, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159979 sha256=40994df8db9321a9656d4e4a848c61d863e072fd1f09a5e58613beac3ae48de0\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=93cc8837976d76d0ae326dd26384fbc638569801bc219af75ac8e7e5ee80d106\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=d82de8081e79ffc0604e972efd2f5258b41ab5307c8b5006670d804fd3199c45\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23393 sha256=1c9f0781b745cc19e11e90c27e4111081db3912dd176bdf19ec29d1a04e353e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=e8e44369286f170d2b0b53e5facea9d28c1bff2a84bc8738c7ea477669e75f8b\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102464 sha256=b5e1093b3aa56212084cea94faa5c412746f5ea429237d9fe18090a507b965f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=2a77368d0feb098ae9a98eee04be2a65b7f82379535edd23677c2c0d48870577\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=6b3af9b8bf4743e3b34018d30c3f307842b65ecb6094dca352a063dfc9971e3f\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp argo-models gast tabulate fire kfp-server-api termcolor strip-hints\n",
      "Installing collected packages: urllib3, cached-property, werkzeug, kubernetes, h5py, absl-py, termcolor, tensorflow-estimator, tensorboard, tabulate, strip-hints, scipy, requests-toolbelt, opt-einsum, kfp-server-api, keras-preprocessing, keras-applications, google-pasta, gast, Deprecated, cloudpickle, click, astor, argo-models, tensorflow, pandas, kfp, fire\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.6\n",
      "    Uninstalling urllib3-1.26.6:\n",
      "      Successfully uninstalled urllib3-1.26.6\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 18.20.0\n",
      "    Uninstalling kubernetes-18.20.0:\n",
      "      Successfully uninstalled kubernetes-18.20.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.1\n",
      "    Uninstalling scipy-1.7.1:\n",
      "      Successfully uninstalled scipy-1.7.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.1\n",
      "    Uninstalling click-8.0.1:\n",
      "      Successfully uninstalled click-8.0.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.2\n",
      "    Uninstalling pandas-1.3.2:\n",
      "      Successfully uninstalled pandas-1.3.2\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.11.2 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.11.2 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "jupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 3.1.0 which is incompatible.\n",
      "black 21.8b0 requires click>=7.1.2, but you have click 7.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 absl-py-0.13.0 argo-models-2.2.1a0 astor-0.8.1 cached-property-1.5.2 click-7.0 cloudpickle-1.1.1 fire-0.4.0 gast-0.2.2 google-pasta-0.2.0 h5py-3.4.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 opt-einsum-3.3.0 pandas-0.24.2 requests-toolbelt-0.9.1 scipy-1.4.1 strip-hints-0.1.10 tabulate-0.8.3 tensorboard-2.1.1 tensorflow-2.1.1 tensorflow-estimator-2.1.0 termcolor-1.1.0 urllib3-1.24.3 werkzeug-2.0.1\n",
      "Removing intermediate container a55cdf32b844\n",
      " ---> b7f599654f7d\n",
      "Successfully built b7f599654f7d\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-00-35d32fccdc52/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-00-35d32fccdc52/base_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-35d32fccdc52/base_image]\n",
      "2aa31b9f3691: Preparing\n",
      "a34559070c4d: Preparing\n",
      "5b1ab540351c: Preparing\n",
      "7e351b6f48b7: Preparing\n",
      "3e4234304903: Preparing\n",
      "fdc0888feca9: Preparing\n",
      "aeae925dc9db: Preparing\n",
      "1bd7873b0779: Preparing\n",
      "376d2b415584: Preparing\n",
      "44f70d274c58: Preparing\n",
      "e7d923b0bae8: Preparing\n",
      "7e2741fe6f4c: Preparing\n",
      "18fbfa48ed76: Preparing\n",
      "51943bd297fe: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "c4594481f563: Preparing\n",
      "97982642ef8a: Preparing\n",
      "251dde02ee49: Preparing\n",
      "6babb56be259: Preparing\n",
      "e7d923b0bae8: Waiting\n",
      "7e2741fe6f4c: Waiting\n",
      "18fbfa48ed76: Waiting\n",
      "51943bd297fe: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "c4594481f563: Waiting\n",
      "97982642ef8a: Waiting\n",
      "251dde02ee49: Waiting\n",
      "6babb56be259: Waiting\n",
      "aeae925dc9db: Waiting\n",
      "1bd7873b0779: Waiting\n",
      "fdc0888feca9: Waiting\n",
      "376d2b415584: Waiting\n",
      "44f70d274c58: Waiting\n",
      "3e4234304903: Layer already exists\n",
      "7e351b6f48b7: Layer already exists\n",
      "5b1ab540351c: Layer already exists\n",
      "a34559070c4d: Layer already exists\n",
      "376d2b415584: Layer already exists\n",
      "1bd7873b0779: Layer already exists\n",
      "aeae925dc9db: Layer already exists\n",
      "fdc0888feca9: Layer already exists\n",
      "18fbfa48ed76: Layer already exists\n",
      "44f70d274c58: Layer already exists\n",
      "7e2741fe6f4c: Layer already exists\n",
      "e7d923b0bae8: Layer already exists\n",
      "c4594481f563: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "97982642ef8a: Layer already exists\n",
      "51943bd297fe: Layer already exists\n",
      "251dde02ee49: Layer already exists\n",
      "6babb56be259: Layer already exists\n",
      "2aa31b9f3691: Pushed\n",
      "latest: digest: sha256:2b20bfcb29df4aa1b7d30d8c5afad359f0fb37da66332d22a0ddad740fc37271 size: 4293\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                    STATUS\n",
      "823c074a-c462-4331-8c1d-9a9debbc6097  2021-09-13T07:04:18+00:00  7M35S     gs://qwiklabs-gcp-00-35d32fccdc52_cloudbuild/source/1631516658.012807-2892ce773dfb4738a23bfda1f37c4703.tgz  gcr.io/qwiklabs-gcp-00-35d32fccdc52/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1355c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "832a0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8a51e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory pipeline: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4272be9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/training_pipeline.py\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "from kfp.components import func_to_container_op\n",
    "from helper_components import retrieve_best_run\n",
    "# import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import json\n",
    "\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "BUCKET = os.getenv('BUCKET')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "\n",
    "TRAINING_DATA_PATH = BUCKET + '/data/dataset.csv'\n",
    "\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 4,\n",
    "        \"maxParallelTrials\": 2,\n",
    "        \"hyperparameterMetricTag\": \"mean_absolute_error)\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"epochs\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"lr\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# Load BigQuery and AI Platform Training op\n",
    "# bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(retrieve_best_run, base_image=BASE_IMAGE)\n",
    "\n",
    "# dsl pipeline definition\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Spanish Demand forecast Continuous Training',\n",
    "    description='Pipeline to create training/validation on AI Platform Training Job'\n",
    ")\n",
    "def pipeline(project_id,\n",
    "             gcs_root,\n",
    "             model_id,\n",
    "             version_id,\n",
    "             replace_existing_version,\n",
    "             region,\n",
    "             hypertune_settings=HYPERTUNE_SETTINGS):\n",
    "\n",
    "    # These are the output directories where our models will be saved\n",
    "    # output_dir = gcs_root + '/models/pipeline'\n",
    "    output_dir = '{}/{}/{}/{}'.format(gcs_root, \n",
    "                                      'models', 'pipeline', \n",
    "                                      kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path', TRAINING_DATA_PATH,\n",
    "        '--hptune', 'True']\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "    \n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(project_id, \n",
    "                                          hypertune.outputs['job_id']).set_display_name('Get best trial')\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    \n",
    "    train_args = [\n",
    "        '--training_dataset_path', TRAINING_DATA_PATH,\n",
    "        '--output_dir', output_dir,\n",
    "        '--window_size', '30',\n",
    "        '--batch_size', '16', \n",
    "        get_best_trial.outputs['lr'], '--lr',\n",
    "        get_best_trial.outputs['epochs'], '--epochs',\n",
    "        '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args).set_display_name('Tensorflow Model Training')\n",
    "    \n",
    "    deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version).set_display_name('Deploy Model')\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "    # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88823004",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = 'latest'\n",
    "TRAINER_IMAGE = 'gcr.io/{}/tensorflow_trainer_image:{}'.format(PROJECT_ID, TAG)\n",
    "BASE_IMAGE='gcr.io/{}/base_image:{}'.format(PROJECT_ID, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd0f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRAINER_IMAGE=gcr.io/qwiklabs-gcp-00-35d32fccdc52/tensorflow_trainer_image:latest\n",
      "env: BASE_IMAGE=gcr.io/qwiklabs-gcp-00-35d32fccdc52/base_image:latest\n",
      "env: BUCKET=gs://qwiklabs-gcp-00-35d32fccdc52\n",
      "env: USE_KFP_SA=False\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env BUCKET={BUCKET}\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a238906",
   "metadata": {},
   "source": [
    "## compile pipeline in a yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd082bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/training_pipeline.py --output pipeline/training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3472c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"apiVersion\": |-\n",
      "  argoproj.io/v1alpha1\n",
      "\"kind\": |-\n",
      "  Workflow\n",
      "\"metadata\":\n",
      "  \"annotations\":\n",
      "    \"pipelines.kubeflow.org/pipeline_spec\": |-\n",
      "      {\"description\": \"Pipeline to create training/validation on AI Platform Training Job\", \"inputs\": [{\"name\": \"project_id\"}, {\"name\": \"gcs_root\"}, {\"name\": \"model_id\"}, {\"name\": \"version_id\"}, {\"name\": \"replace_existing_version\"}, {\"name\": \"region\"}, {\"default\": \"\\n{\\n    \\\"hyperparameters\\\":  {\\n        \\\"goal\\\": \\\"MAXIMIZE\\\",\\n        \\\"maxTrials\\\": 4,\\n        \\\"maxParallelTrials\\\": 2,\\n        \\\"hyperparameterMetricTag\\\": \\\"mean_absolute_error)\\\",\\n        \\\"enableTrialEarlyStopping\\\": True,\\n        \\\"params\\\": [\\n            {\\n                \\\"parameterName\\\": \\\"epochs\\\",\\n                \\\"type\\\": \\\"DISCRETE\\\",\\n                \\\"discreteValues\\\": [500, 1000]\\n            },\\n            {\\n                \\\"parameterName\\\": \\\"lr\\\",\\n                \\\"type\\\": \\\"DOUBLE\\\",\\n                \\\"minValue\\\": 0.0001,\\n                \\\"maxValue\\\": 0.001,\\n                \\\"scaleType\\\": \\\"UNIT_LINEAR_SCALE\\\"\\n            }\\n        ]\\n    }\\n\", \"name\": \"hypertune_settings\", \"optional\": true}], \"name\": \"Spanish Demand forecast Continuous Training\"}\n",
      "  \"generateName\": |-\n",
      "    spanish-demand-forecast-continuous-training-\n"
     ]
    }
   ],
   "source": [
    "!head pipeline/training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6851894",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = '4a00ac13a87686cc-dot-us-central1.pipelines.googleusercontent.com'\n",
    "PIPELINE_NAME = 'demand_predictor_pipeline_model_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d957c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0e0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 28034510-a334-4247-a9c3-e622811e9687 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           28034510-a334-4247-a9c3-e622811e9687\n",
      "Name         demand_predictor_pipeline_model_v2\n",
      "Description\n",
      "Uploaded at  2021-09-13T08:03:12+00:00\n",
      "+--------------------------+------------------------------------------------------------+\n",
      "| Parameter Name           | Default Value                                              |\n",
      "+==========================+============================================================+\n",
      "| project_id               |                                                            |\n",
      "+--------------------------+------------------------------------------------------------+\n",
      "| gcs_root                 |                                                            |\n",
      "+--------------------------+------------------------------------------------------------+\n",
      "| model_id                 |                                                            |\n",
      "+--------------------------+------------------------------------------------------------+\n",
      "| version_id               |                                                            |\n",
      "+--------------------------+------------------------------------------------------------+\n",
      "| replace_existing_version |                                                            |\n",
      "+--------------------------+------------------------------------------------------------+\n",
      "| region                   |                                                            |\n",
      "+--------------------------+------------------------------------------------------------+\n",
      "| hypertune_settings       | {                                                          |\n",
      "|                          |     \"hyperparameters\":  {                                  |\n",
      "|                          |         \"goal\": \"MAXIMIZE\",                                |\n",
      "|                          |         \"maxTrials\": 4,                                    |\n",
      "|                          |         \"maxParallelTrials\": 2,                            |\n",
      "|                          |         \"hyperparameterMetricTag\": \"mean_absolute_error)\", |\n",
      "|                          |         \"enableTrialEarlyStopping\": True,                  |\n",
      "|                          |         \"params\": [                                        |\n",
      "|                          |             {                                              |\n",
      "|                          |                 \"parameterName\": \"epochs\",                 |\n",
      "|                          |                 \"type\": \"DISCRETE\",                        |\n",
      "|                          |                 \"discreteValues\": [500, 1000]              |\n",
      "|                          |             },                                             |\n",
      "|                          |             {                                              |\n",
      "|                          |                 \"parameterName\": \"lr\",                     |\n",
      "|                          |                 \"type\": \"DOUBLE\",                          |\n",
      "|                          |                 \"minValue\": 0.0001,                        |\n",
      "|                          |                 \"maxValue\": 0.001,                         |\n",
      "|                          |                 \"scaleType\": \"UNIT_LINEAR_SCALE\"           |\n",
      "|                          |             }                                              |\n",
      "|                          |         ]                                                  |\n",
      "|                          |     }                                                      |\n",
      "+--------------------------+------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "./pipeline/training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ac2521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                           | Uploaded at               |\n",
      "+======================================+================================================+===========================+\n",
      "| 28034510-a334-4247-a9c3-e622811e9687 | demand_predictor_pipeline_model_v2             | 2021-09-13T08:03:12+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| cf36bc23-dfb6-4995-8558-3eab9e51398e | demand_predictor_pipeline_model                | 2021-09-13T07:50:01+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 8fb0afdf-ae56-402f-94b0-9ae2b2ad616a | [Tutorial] V2 lightweight Python components    | 2021-09-13T07:06:07+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 97a3acce-1e34-4438-96db-f98f06c3b700 | [Tutorial] DSL - Control structures            | 2021-09-13T07:06:06+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 8f081cad-3422-45c8-8826-4815cacf6b56 | [Tutorial] Data passing in python components   | 2021-09-13T07:06:05+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 5725f510-fc15-489d-a497-d29ebf9e45b3 | [Demo] TFX - Taxi tip prediction model trainer | 2021-09-13T07:06:04+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n",
      "| 42c3f5e0-d4c3-4368-b27d-684e6748f16f | [Demo] XGBoost - Iterative model training      | 2021-09-13T07:06:03+00:00 |\n",
      "+--------------------------------------+------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc058088",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='0918568d-758c-46cf-9752-e04a4403cd84' # TO DO: REPLACE WITH YOUR PIPELINE ID\n",
    "EXPERIMENT_NAME = 'Covertype_Classifier_Training'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
