{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ca7a31",
   "metadata": {},
   "source": [
    "# TF in a KFP running in AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0daf799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tensorflow_trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/train.py\n",
    "\n",
    "\"\"\"Tensorflow predictor script.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import fire\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(pattern, window_size=30, batch_size=16, shuffle_buffer=100):\n",
    "    \"\"\"\n",
    "    Description:  \n",
    "    Input: \n",
    "      - series:\n",
    "      - window_size:\n",
    "      - batch_size: the batches to use when training\n",
    "      -shuffle_buffer: size buffer, how data will be shuffled\n",
    "\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # read data\n",
    "    data = pd.read_csv(pattern)\n",
    "    time = np.array(data.times)\n",
    "    series = np.array(data.values)[:,1].astype('float32')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1])) # x and y (last one)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset, series\n",
    "\n",
    "def train_evaluate(training_dataset_path, \n",
    "                   # validation_dataset_path,\n",
    "                   window_size,\n",
    "                   batch_size,\n",
    "                   epochs,\n",
    "                   lr,\n",
    "                   # num_train_examples, num_evals, \n",
    "                   output_dir):\n",
    "    \"\"\"\n",
    "    Description: train script\n",
    "    \"\"\"\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    LR = lr\n",
    "    \n",
    "    l0 = tf.keras.layers.Dense(2*window_size+1, input_shape=[window_size], activation='relu')\n",
    "    l2 = tf.keras.layers.Dense(1)\n",
    "    model = tf.keras.models.Sequential([l0, l2])\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3)\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=LR, momentum=0.9)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    # load data\n",
    "    (trainds,series) = load_dataset(pattern=training_dataset_path, window_size=window_size, batch_size=batch_size)\n",
    "    # evalds = load_dataset(pattern=validation_dataset_path, mode='eval')\n",
    "    \n",
    "    history = model.fit(trainds, epochs=EPOCHS, verbose=0)\n",
    "       \n",
    "    if hptune:\n",
    "        # trainds = load_dataset(pattern=training_dataset_path, window_size=window_size, batch_size=batch_size)\n",
    "        results = np.array(model.predict(trainds))[:, 0]\n",
    "        mae = tf.keras.metrics.mean_absolute_error(series[window_size:], results).numpy()  \n",
    "        print('Model accuracy: {}'.format(mae))# Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='mean_absolute_error', metric_value=mae)\n",
    "               \n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        EXPORT_PATH = os.path.join(output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        tf.saved_model.save(obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "        \n",
    "        # model_filename = 'model.pkl'\n",
    "        # with open(model_filename, 'wb') as model_file:\n",
    "            # pickle.dump(pipeline, model_file)\n",
    "            # gcs_model_path = '{}/{}'.format(job_dir, model_filename)\n",
    "            # subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        #print('Saved model in: {}'.format(gcs_model_path))\n",
    "        \n",
    "        print(\"Exported trained model to {}\".format(EXPORT_PATH))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4b49536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tensorflow_trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire tensorflow==2.1.1\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "948557cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IMAGE_NAME='tensorflow_trainer_image'\n",
    "TF_IMAGE_TAG='latest'\n",
    "TF_IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, TF_IMAGE_NAME, TF_IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de855838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 3.4 KiB before compression.\n",
      "Uploading tarball of [tensorflow_trainer_image] to [gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631464725.646652-371286ff8f1c4d81919843c782dff552.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-01-43b0d7048e07/locations/global/builds/84206eb5-c80a-40d0-abf3-a48324cf83fb].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/84206eb5-c80a-40d0-abf3-a48324cf83fb?project=821318692321].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"84206eb5-c80a-40d0-abf3-a48324cf83fb\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631464725.646652-371286ff8f1c4d81919843c782dff552.tgz#1631464725901368\n",
      "Copying gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631464725.646652-371286ff8f1c4d81919843c782dff552.tgz#1631464725901368...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "04d61753a965: Pulling fs layer\n",
      "4c841a8dc1c2: Pulling fs layer\n",
      "ba19310bc52a: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "187fc16bda8d: Pulling fs layer\n",
      "be760be104b7: Pulling fs layer\n",
      "ba19310bc52a: Waiting\n",
      "67ef79a8e023: Pulling fs layer\n",
      "31110891cfb1: Pulling fs layer\n",
      "8c8c23a31282: Pulling fs layer\n",
      "b1bb999ddb16: Pulling fs layer\n",
      "3ce4958fd428: Pulling fs layer\n",
      "77615062896e: Pulling fs layer\n",
      "a3ad053dcd2a: Pulling fs layer\n",
      "60b86c84dfcb: Pulling fs layer\n",
      "bb2debff4a41: Pulling fs layer\n",
      "502011482b4c: Pulling fs layer\n",
      "26c66a4831b0: Pulling fs layer\n",
      "187fc16bda8d: Waiting\n",
      "be760be104b7: Waiting\n",
      "67ef79a8e023: Waiting\n",
      "31110891cfb1: Waiting\n",
      "8c8c23a31282: Waiting\n",
      "b1bb999ddb16: Waiting\n",
      "3ce4958fd428: Waiting\n",
      "77615062896e: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "a3ad053dcd2a: Waiting\n",
      "60b86c84dfcb: Waiting\n",
      "bb2debff4a41: Waiting\n",
      "502011482b4c: Waiting\n",
      "26c66a4831b0: Waiting\n",
      "04d61753a965: Verifying Checksum\n",
      "04d61753a965: Download complete\n",
      "e4ca327ec0e7: Verifying Checksum\n",
      "e4ca327ec0e7: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "187fc16bda8d: Verifying Checksum\n",
      "187fc16bda8d: Download complete\n",
      "ba19310bc52a: Verifying Checksum\n",
      "ba19310bc52a: Download complete\n",
      "67ef79a8e023: Verifying Checksum\n",
      "67ef79a8e023: Download complete\n",
      "31110891cfb1: Verifying Checksum\n",
      "31110891cfb1: Download complete\n",
      "8c8c23a31282: Verifying Checksum\n",
      "8c8c23a31282: Download complete\n",
      "b1bb999ddb16: Verifying Checksum\n",
      "b1bb999ddb16: Download complete\n",
      "3ce4958fd428: Verifying Checksum\n",
      "3ce4958fd428: Download complete\n",
      "77615062896e: Verifying Checksum\n",
      "77615062896e: Download complete\n",
      "a3ad053dcd2a: Verifying Checksum\n",
      "a3ad053dcd2a: Download complete\n",
      "60b86c84dfcb: Verifying Checksum\n",
      "60b86c84dfcb: Download complete\n",
      "bb2debff4a41: Verifying Checksum\n",
      "bb2debff4a41: Download complete\n",
      "be760be104b7: Verifying Checksum\n",
      "be760be104b7: Download complete\n",
      "26c66a4831b0: Verifying Checksum\n",
      "26c66a4831b0: Download complete\n",
      "4c841a8dc1c2: Verifying Checksum\n",
      "4c841a8dc1c2: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "04d61753a965: Pull complete\n",
      "502011482b4c: Verifying Checksum\n",
      "502011482b4c: Download complete\n",
      "4c841a8dc1c2: Pull complete\n",
      "ba19310bc52a: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "187fc16bda8d: Pull complete\n",
      "be760be104b7: Pull complete\n",
      "67ef79a8e023: Pull complete\n",
      "31110891cfb1: Pull complete\n",
      "8c8c23a31282: Pull complete\n",
      "b1bb999ddb16: Pull complete\n",
      "3ce4958fd428: Pull complete\n",
      "77615062896e: Pull complete\n",
      "a3ad053dcd2a: Pull complete\n",
      "60b86c84dfcb: Pull complete\n",
      "bb2debff4a41: Pull complete\n",
      "502011482b4c: Pull complete\n",
      "26c66a4831b0: Pull complete\n",
      "Digest: sha256:d13107221c97d56ffa891a44beaecaa8438c87102ec7e399f97c77d70c7cd7e4\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> b99046b5f2b5\n",
      "Step 2/5 : RUN pip install -U fire tensorflow==2.1.1\n",
      " ---> Running in a2e6eeac048b\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting tensorflow==2.1.1\n",
      "  Downloading tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (3.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.38.1)\n",
      "Collecting keras-preprocessing==1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.12.1)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (0.37.0)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (57.4.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.25.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.1.1)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.5.0)\n",
      "Building wheels for collected packages: gast, fire, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=c8492262a0f44b9b1d0c0668c957044ac994a6535d99e772b9d0853f2d997bce\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=e8f8f76d9ee2564f4cc75b3f464a71586ffbf1e50a6a28811cae512e5a9204ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=4911910667d8193dcc139506f3cad61273e85846a9791e7d5d9851ab807c331f\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built gast fire termcolor\n",
      "Installing collected packages: cached-property, werkzeug, h5py, absl-py, termcolor, tensorflow-estimator, tensorboard, scipy, opt-einsum, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow, fire\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.1\n",
      "    Uninstalling scipy-1.7.1:\n",
      "      Successfully uninstalled scipy-1.7.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "phik 0.11.2 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "\u001b[0mSuccessfully installed absl-py-0.13.0 astor-0.8.1 cached-property-1.5.2 fire-0.4.0 gast-0.2.2 google-pasta-0.2.0 h5py-3.4.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 opt-einsum-3.3.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.1 tensorflow-estimator-2.1.0 termcolor-1.1.0 werkzeug-2.0.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container a2e6eeac048b\n",
      " ---> bc6ab8c51d44\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in d948395cc636\n",
      "Removing intermediate container d948395cc636\n",
      " ---> 6d4cc7823ffd\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 96a0b8d9c818\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 600f2aaa71dc\n",
      "Removing intermediate container 600f2aaa71dc\n",
      " ---> 8610c5ed02a6\n",
      "Successfully built 8610c5ed02a6\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image]\n",
      "d1c7dbcd984a: Preparing\n",
      "1c901e821447: Preparing\n",
      "7526c24656e2: Preparing\n",
      "a34559070c4d: Preparing\n",
      "5b1ab540351c: Preparing\n",
      "7e351b6f48b7: Preparing\n",
      "3e4234304903: Preparing\n",
      "fdc0888feca9: Preparing\n",
      "aeae925dc9db: Preparing\n",
      "1bd7873b0779: Preparing\n",
      "376d2b415584: Preparing\n",
      "44f70d274c58: Preparing\n",
      "e7d923b0bae8: Preparing\n",
      "7e2741fe6f4c: Preparing\n",
      "18fbfa48ed76: Preparing\n",
      "51943bd297fe: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "c4594481f563: Preparing\n",
      "97982642ef8a: Preparing\n",
      "251dde02ee49: Preparing\n",
      "6babb56be259: Preparing\n",
      "e7d923b0bae8: Waiting\n",
      "7e2741fe6f4c: Waiting\n",
      "18fbfa48ed76: Waiting\n",
      "51943bd297fe: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "c4594481f563: Waiting\n",
      "97982642ef8a: Waiting\n",
      "251dde02ee49: Waiting\n",
      "6babb56be259: Waiting\n",
      "7e351b6f48b7: Waiting\n",
      "3e4234304903: Waiting\n",
      "fdc0888feca9: Waiting\n",
      "aeae925dc9db: Waiting\n",
      "1bd7873b0779: Waiting\n",
      "376d2b415584: Waiting\n",
      "44f70d274c58: Waiting\n",
      "a34559070c4d: Layer already exists\n",
      "5b1ab540351c: Layer already exists\n",
      "7e351b6f48b7: Layer already exists\n",
      "3e4234304903: Layer already exists\n",
      "fdc0888feca9: Layer already exists\n",
      "aeae925dc9db: Layer already exists\n",
      "1bd7873b0779: Layer already exists\n",
      "376d2b415584: Layer already exists\n",
      "44f70d274c58: Layer already exists\n",
      "e7d923b0bae8: Layer already exists\n",
      "7e2741fe6f4c: Layer already exists\n",
      "18fbfa48ed76: Layer already exists\n",
      "51943bd297fe: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "97982642ef8a: Layer already exists\n",
      "c4594481f563: Layer already exists\n",
      "251dde02ee49: Layer already exists\n",
      "6babb56be259: Layer already exists\n",
      "d1c7dbcd984a: Pushed\n",
      "1c901e821447: Pushed\n",
      "7526c24656e2: Pushed\n",
      "latest: digest: sha256:5819525827c13ac9bc21b9b71b8c87cee7129d1ad85ac21967a7d17933ad2c18 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                  STATUS\n",
      "84206eb5-c80a-40d0-abf3-a48324cf83fb  2021-09-12T16:38:46+00:00  6M23S     gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631464725.646652-371286ff8f1c4d81919843c782dff552.tgz  gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TF_IMAGE_URI $TF_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d446172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./base_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./base_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire tensorflow==2.1.1 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23b95141",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 120 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631466147.099502-bd0f54c9d7494ae686fb8d42db025850.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-01-43b0d7048e07/locations/global/builds/0ce1e026-c759-4a6f-90d4-2638e469c668].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/0ce1e026-c759-4a6f-90d4-2638e469c668?project=821318692321].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"0ce1e026-c759-4a6f-90d4-2638e469c668\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631466147.099502-bd0f54c9d7494ae686fb8d42db025850.tgz#1631466147370820\n",
      "Copying gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631466147.099502-bd0f54c9d7494ae686fb8d42db025850.tgz#1631466147370820...\n",
      "/ [1 files][  229.0 B/  229.0 B]                                                \n",
      "Operation completed over 1 objects/229.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "04d61753a965: Pulling fs layer\n",
      "4c841a8dc1c2: Pulling fs layer\n",
      "ba19310bc52a: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "187fc16bda8d: Pulling fs layer\n",
      "be760be104b7: Pulling fs layer\n",
      "67ef79a8e023: Pulling fs layer\n",
      "31110891cfb1: Pulling fs layer\n",
      "8c8c23a31282: Pulling fs layer\n",
      "b1bb999ddb16: Pulling fs layer\n",
      "3ce4958fd428: Pulling fs layer\n",
      "77615062896e: Pulling fs layer\n",
      "a3ad053dcd2a: Pulling fs layer\n",
      "60b86c84dfcb: Pulling fs layer\n",
      "bb2debff4a41: Pulling fs layer\n",
      "502011482b4c: Pulling fs layer\n",
      "26c66a4831b0: Pulling fs layer\n",
      "ba19310bc52a: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "b1bb999ddb16: Waiting\n",
      "187fc16bda8d: Waiting\n",
      "3ce4958fd428: Waiting\n",
      "be760be104b7: Waiting\n",
      "77615062896e: Waiting\n",
      "67ef79a8e023: Waiting\n",
      "31110891cfb1: Waiting\n",
      "a3ad053dcd2a: Waiting\n",
      "8c8c23a31282: Waiting\n",
      "bb2debff4a41: Waiting\n",
      "502011482b4c: Waiting\n",
      "26c66a4831b0: Waiting\n",
      "04d61753a965: Verifying Checksum\n",
      "04d61753a965: Download complete\n",
      "e4ca327ec0e7: Verifying Checksum\n",
      "e4ca327ec0e7: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "187fc16bda8d: Verifying Checksum\n",
      "187fc16bda8d: Download complete\n",
      "ba19310bc52a: Verifying Checksum\n",
      "ba19310bc52a: Download complete\n",
      "67ef79a8e023: Verifying Checksum\n",
      "67ef79a8e023: Download complete\n",
      "31110891cfb1: Verifying Checksum\n",
      "31110891cfb1: Download complete\n",
      "8c8c23a31282: Verifying Checksum\n",
      "8c8c23a31282: Download complete\n",
      "b1bb999ddb16: Verifying Checksum\n",
      "b1bb999ddb16: Download complete\n",
      "3ce4958fd428: Download complete\n",
      "77615062896e: Verifying Checksum\n",
      "77615062896e: Download complete\n",
      "a3ad053dcd2a: Verifying Checksum\n",
      "a3ad053dcd2a: Download complete\n",
      "60b86c84dfcb: Verifying Checksum\n",
      "60b86c84dfcb: Download complete\n",
      "bb2debff4a41: Verifying Checksum\n",
      "bb2debff4a41: Download complete\n",
      "be760be104b7: Verifying Checksum\n",
      "be760be104b7: Download complete\n",
      "26c66a4831b0: Verifying Checksum\n",
      "26c66a4831b0: Download complete\n",
      "4c841a8dc1c2: Verifying Checksum\n",
      "4c841a8dc1c2: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "04d61753a965: Pull complete\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd41fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e88aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d64e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-01-43b0d7048e07\n",
      "gs://qwiklabs-gcp-01-43b0d7048e07/data/dataset.csv\n",
      "gs://qwiklabs-gcp-01-43b0d7048e07/models\n"
     ]
    }
   ],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BUCKET = 'gs://' + PROJECT_ID\n",
    "print(BUCKET)\n",
    "\n",
    "ARTIFACT_STORE = BUCKET # + 'kubeflowpipelines-default'\n",
    "\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}'.format(DATA_ROOT, 'dataset.csv')\n",
    "# VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')\n",
    "print(TRAINING_FILE_PATH)\n",
    "OUTPUT_DIR = '{}/models'.format(ARTIFACT_STORE)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c601a479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/training_pipeline.py\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "from kfp.components import func_to_container_op\n",
    "from helper_components import retrieve_best_run\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import json\n",
    "\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = \n",
    "BUCKET = os.getenv('BUCKET')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "\n",
    "TRAINING_DATA_PATH = BUCKET + '/data/dataset.csv'\n",
    "\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 4,\n",
    "        \"maxParallelTrials\": 2,\n",
    "        \"hyperparameterMetricTag\": \"mean_absolute_error)\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"epochs\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"lr\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# Load BigQuery and AI Platform Training op\n",
    "# bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(retrieve_best_run, base_image=BASE_IMAGE)\n",
    "\n",
    "# dsl pipeline definition\n",
    "@dsl.pipeline(\n",
    "    name='Spanish Demand forecast Continuous Training',\n",
    "    description='Pipeline to create training/validation on AI Platform Training Job'\n",
    ")\n",
    "def pipeline(project_id,\n",
    "             gcs_root,\n",
    "             model_id,\n",
    "             version_id,\n",
    "             replace_existing_version,\n",
    "             region='us-central1',\n",
    "             hypertune_settings=HYPERTUNE_SETTINGS):\n",
    "\n",
    "    # These are the output directories where our models will be saved\n",
    "    output_dir = project_id + '/models/pipeline'\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path', TRAINING_DATA_PATH,\n",
    "        '--hptune', 'True']\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "    \n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    \n",
    "    train_args = [\n",
    "        '--training_dataset_path', TRAINING_DATA_PATH,\n",
    "        '--output_dir', output_dir,\n",
    "        '--window_size', '30',\n",
    "        '--batch_size', '16', \n",
    "        get_best_trial.outputs['lr'], '--lr',\n",
    "        get_best_trial.outputs['epochs'], '--epochs'\n",
    "        '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args).set_display_name('Tensorflow Model Training')\n",
    "    \n",
    "    deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "    # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "348831ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = 'latest'\n",
    "TRAINER_IMAGE = 'gcr.io/{}/tensorflow_trainer_image:{}'.format(PROJECT_ID, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a37e6b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRAINER_IMAGE=gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image:latest\n",
      "env: BUCKET=gs://qwiklabs-gcp-01-43b0d7048e07\n",
      "env: USE_KFP_SA=False\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env BUCKET={BUCKET}\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466148dd",
   "metadata": {},
   "source": [
    "## compile pipeline in a yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32663e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/dsl-compile\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/compiler/main.py\", line 123, in main\n",
      "    compile_pyfile(args.py, args.function, args.output, not args.disable_type_check)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/compiler/main.py\", line 111, in compile_pyfile\n",
      "    __import__(os.path.splitext(filename)[0])\n",
      "  File \"pipeline/training_pipeline.py\", line 10, in <module>\n",
      "    from helper_components import evaluate_model\n",
      "ModuleNotFoundError: No module named 'helper_components'\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile --py pipeline/training_pipeline.py --output training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b7f5e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"apiVersion\": |-\n",
      "  argoproj.io/v1alpha1\n",
      "\"kind\": |-\n",
      "  Workflow\n",
      "\"metadata\":\n",
      "  \"annotations\":\n",
      "    \"pipelines.kubeflow.org/pipeline_spec\": |-\n",
      "      {\"description\": \"Pipeline to create training/validation on AI Platform Training Job\", \"inputs\": [{\"name\": \"project_id\"}, {\"default\": \"us-central1\", \"name\": \"region\", \"optional\": true}], \"name\": \"Spanish Demand forecast Continuous Training\"}\n",
      "  \"generateName\": |-\n",
      "    spanish-demand-forecast-continuous-training-\n"
     ]
    }
   ],
   "source": [
    "!head training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f4e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = '4cd7a2009b126fb2-dot-us-central1.pipelines.googleusercontent.com'\n",
    "PIPELINE_NAME = 'demand_predictor_pipeline_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20076658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 58dd8b93-0002-4099-9337-cfef63444eb6 has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           58dd8b93-0002-4099-9337-cfef63444eb6\n",
      "Name         demand_predictor_pipeline_model\n",
      "Description\n",
      "Uploaded at  2021-09-12T15:42:08+00:00\n",
      "+------------------+-----------------+\n",
      "| Parameter Name   | Default Value   |\n",
      "+==================+=================+\n",
      "| project_id       |                 |\n",
      "+------------------+-----------------+\n",
      "| region           | us-central1     |\n",
      "+------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "./training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9a075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
