name: Classif model eval metrics
description: This function renders evaluation metrics for an AutoML Tabular classification
  model.
inputs:
- {name: project, type: String}
- {name: location, type: String}
- {name: api_endpoint, type: String}
- {name: thresholds_dict_str, type: String}
- {name: model, type: Model}
outputs:
- {name: metrics, type: Metrics}
- {name: metricsc, type: ClassificationMetrics}
- {name: dep_decision, type: String}
implementation:
  container:
    image: gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest
    command:
    - sh
    - -c
    - (python3 -m ensurepip || python3 -m ensurepip --user) && (PIP_DISABLE_PIP_VERSION_CHECK=1
      python3 -m pip install --quiet                 --no-warn-script-location 'google-cloud-aiplatform'
      'kfp==1.8.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
      'google-cloud-aiplatform' 'kfp==1.8.2' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef classif_model_eval_metrics(\n\
      \    project: str,\n    location: str,  # \"us-central1\",\n    api_endpoint:\
      \ str,  # \"us-central1-aiplatform.googleapis.com\",\n    thresholds_dict_str:\
      \ str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc:\
      \ Output[ClassificationMetrics],\n) -> NamedTuple(\"Outputs\", [(\"dep_decision\"\
      , str)]):  # Return parameter.\n\n    \"\"\"This function renders evaluation\
      \ metrics for an AutoML Tabular classification model.\n    It retrieves the\
      \ classification model evaluation generated by the AutoML Tabular training\n\
      \    process, does some parsing, and uses that info to render the ROC curve\
      \ and confusion matrix\n    for the model. It also uses given metrics threshold\
      \ information and compares that to the\n    evaluation results to determine\
      \ whether the model is sufficiently accurate to deploy.\n    \"\"\"\n    import\
      \ json\n    import logging\n\n    from google.cloud import aiplatform\n\n  \
      \  # Fetch model eval info\n    def get_eval_info(client, model_name):\n   \
      \     from google.protobuf.json_format import MessageToDict\n\n        response\
      \ = client.list_model_evaluations(parent=model_name)\n        metrics_list =\
      \ []\n        metrics_string_list = []\n        for evaluation in response:\n\
      \            print(\"model_evaluation\")\n            print(\" name:\", evaluation.name)\n\
      \            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n\
      \            metrics = MessageToDict(evaluation._pb.metrics)\n            for\
      \ metric in metrics.keys():\n                logging.info(\"metric: %s, value:\
      \ %s\", metric, metrics[metric])\n            metrics_str = json.dumps(metrics)\n\
      \            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n\
      \n        return (\n            evaluation.name,\n            metrics_list,\n\
      \            metrics_string_list,\n        )\n\n    # Use the given metrics\
      \ threshold(s) to determine whether the model is \n    # accurate enough to\
      \ deploy.\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n\
      \        for k, v in thresholds_dict.items():\n            logging.info(\"k\
      \ {}, v {}\".format(k, v))\n            if k in [\"auRoc\", \"auPrc\"]:  # higher\
      \ is better\n                if metrics_dict[k] < v:  # if under threshold,\
      \ don't deploy\n                    logging.info(\n                        \"\
      {} < {}; returning False\".format(metrics_dict[k], v)\n                    )\n\
      \                    return False\n        logging.info(\"threshold checks passed.\"\
      )\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n   \
      \     test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n       \
      \ logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n\n        # log\
      \ the ROC curve\n        fpr = []\n        tpr = []\n        thresholds = []\n\
      \        for item in metrics_list[0][\"confidenceMetrics\"]:\n            fpr.append(item.get(\"\
      falsePositiveRate\", 0.0))\n            tpr.append(item.get(\"recall\", 0.0))\n\
      \            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n   \
      \     print(f\"fpr: {fpr}\")\n        print(f\"tpr: {tpr}\")\n        print(f\"\
      thresholds: {thresholds}\")\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n\
      \n        # log the confusion matrix\n        annotations = []\n        for\
      \ item in test_confusion_matrix[\"annotationSpecs\"]:\n            annotations.append(item[\"\
      displayName\"])\n        logging.info(\"confusion matrix annotations: %s\",\
      \ annotations)\n        metricsc.log_confusion_matrix(\n            annotations,\n\
      \            test_confusion_matrix[\"rows\"],\n        )\n\n        # log textual\
      \ metrics info as well\n        for metric in metrics_list[0].keys():\n    \
      \        if metric != \"confidenceMetrics\":\n                val_string = json.dumps(metrics_list[0][metric])\n\
      \                metrics.log_metric(metric, val_string)\n        # metrics.metadata[\"\
      model_type\"] = \"AutoML Tabular classification\"\n\n    logging.getLogger().setLevel(logging.INFO)\n\
      \    aiplatform.init(project=project)\n    # extract the model resource name\
      \ from the input Model Artifact\n    model_resource_path = model.uri.replace(\"\
      aiplatform://v1/\", \"\")\n    logging.info(\"model path: %s\", model_resource_path)\n\
      \n    client_options = {\"api_endpoint\": api_endpoint}\n    # Initialize client\
      \ that will be used to create and send requests.\n    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n\
      \    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client,\
      \ model_resource_path\n    )\n    logging.info(\"got evaluation name: %s\",\
      \ eval_name)\n    logging.info(\"got metrics list: %s\", metrics_list)\n   \
      \ log_metrics(metrics_list, metricsc)\n\n    thresholds_dict = json.loads(thresholds_dict_str)\n\
      \    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n\
      \    if deploy:\n        dep_decision = \"true\"\n    else:\n        dep_decision\
      \ = \"false\"\n    logging.info(\"deployment decision is %s\", dep_decision)\n\
      \n    return (dep_decision,)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - classif_model_eval_metrics
