{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f279b43b",
   "metadata": {},
   "source": [
    "# Using custom containers with AI Platform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09124f5c",
   "metadata": {},
   "source": [
    "Objetives: \n",
    "\n",
    "1. ¿? how to create a train and a validation split with BigQuery\n",
    "2. how to wrap a machine learning model into a Docker container and train in on AI Platform\n",
    "3. Learn how to use the hyperparameter tunning engine on Google Cloud to find the best hyperparameters\n",
    "4. Learn how to deploy a trained machine learning model Google Cloud as a rest API and query it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68fd05",
   "metadata": {},
   "source": [
    "Main steps:\n",
    "\n",
    "1. Create the training script\n",
    "2. Package training script into a Docker Image\n",
    "3. Build and push training image to Google Cloud Container Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84104f0c",
   "metadata": {},
   "source": [
    "tricks shell!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1d000",
   "metadata": {},
   "source": [
    "export PROJECT_ID=$(gcloud config get-value core/project)\n",
    "gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6c53a",
   "metadata": {},
   "source": [
    "gcloud services enable \\\n",
    "cloudbuild.googleapis.com \\\n",
    "container.googleapis.com \\\n",
    "cloudresourcemanager.googleapis.com \\\n",
    "iam.googleapis.com \\\n",
    "containerregistry.googleapis.com \\\n",
    "containeranalysis.googleapis.com \\\n",
    "ml.googleapis.com \\\n",
    "dataflow.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1017c25",
   "metadata": {},
   "source": [
    "PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\")\n",
    "CLOUD_BUILD_SERVICE_ACCOUNT=\"${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "  --member serviceAccount:$CLOUD_BUILD_SERVICE_ACCOUNT \\\n",
    "  --role roles/editor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b53cd",
   "metadata": {},
   "source": [
    "### create a GKE in a shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cfdab",
   "metadata": {},
   "source": [
    "gcloud container clusters create cluster-1 --zone us-central1-a --cluster-version 1.18.20 --machine-type n1-standard-2 --enable-basic-auth --scopes=https://www.googleapis.com/auth/cloud-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16cad8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8dea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-01-43b0d7048e07\n",
      "gs://qwiklabs-gcp-01-43b0d7048e07/data/dataset.csv\n",
      "gs://qwiklabs-gcp-01-43b0d7048e07/models\n"
     ]
    }
   ],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BUCKET = 'gs://' + PROJECT_ID\n",
    "print(BUCKET)\n",
    "\n",
    "ARTIFACT_STORE = BUCKET # + 'kubeflowpipelines-default'\n",
    "\n",
    "DATA_ROOT='{}/data'.format(ARTIFACT_STORE)\n",
    "JOB_DIR_ROOT='{}/jobs'.format(ARTIFACT_STORE)\n",
    "TRAINING_FILE_PATH='{}/{}'.format(DATA_ROOT, 'dataset.csv')\n",
    "# VALIDATION_FILE_PATH='{}/{}/{}'.format(DATA_ROOT, 'validation', 'dataset.csv')\n",
    "print(TRAINING_FILE_PATH)\n",
    "OUTPUT_DIR = '{}/models'.format(ARTIFACT_STORE)\n",
    "print(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82677e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/data.json [Content-Type=application/json]...\n",
      "Copying file://data/dataset_eval.csv [Content-Type=text/csv]...                 \n",
      "Copying file://data/dataset.csv [Content-Type=text/csv]...                      \n",
      "/ [3 files][ 40.4 KiB/ 40.4 KiB]                                                \n",
      "Operation completed over 3 objects/40.4 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r data gs://qwiklabs-gcp-01-43b0d7048e07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea0841c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘tensorflow_trainer_image’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir tensorflow_trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac38060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tensorflow_trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/train.py\n",
    "\n",
    "\"\"\"Tensorflow predictor script.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import fire\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(pattern, window_size=30, batch_size=16, shuffle_buffer=100):\n",
    "    \"\"\"\n",
    "    Description:  \n",
    "    Input: \n",
    "      - series:\n",
    "      - window_size:\n",
    "      - batch_size: the batches to use when training\n",
    "      -shuffle_buffer: size buffer, how data will be shuffled\n",
    "\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # read data\n",
    "    data = pd.read_csv(pattern)\n",
    "    time = np.array(data.times)\n",
    "    series = np.array(data.values)[:,1].astype('float32')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1])) # x and y (last one)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "def train_evaluate(training_dataset_path, \n",
    "                   # validation_dataset_path,\n",
    "                   window_size,\n",
    "                   batch_size,\n",
    "                   epochs,\n",
    "                   lr,\n",
    "                   # num_train_examples, num_evals, \n",
    "                   output_dir):\n",
    "    \"\"\"\n",
    "    Description: train script\n",
    "    \"\"\"\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    LR = lr\n",
    "    \n",
    "    l0 = tf.keras.layers.Dense(2*window_size+1, input_shape=[window_size], activation='relu')\n",
    "    l2 = tf.keras.layers.Dense(1)\n",
    "    model = tf.keras.models.Sequential([l0, l2])\n",
    "    \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3)\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=LR, momentum=0.9)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    # load data\n",
    "    trainds = load_dataset(pattern=training_dataset_path, window_size=window_size, batch_size=batch_size)\n",
    "    # evalds = load_dataset(pattern=validation_dataset_path, mode='eval')\n",
    "    \n",
    "    history = model.fit(trainds, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    EXPORT_PATH = os.path.join(output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c036f",
   "metadata": {},
   "source": [
    "## Package TensorFlow Training Script into a Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c70e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tensorflow_trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire tensorflow==2.1.1\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1b956",
   "metadata": {},
   "source": [
    "## Build the Tensorflow Trainer Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6414fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IMAGE_NAME='tensorflow_trainer_image'\n",
    "TF_IMAGE_TAG='latest'\n",
    "TF_IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, TF_IMAGE_NAME, TF_IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83bb0d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.5 KiB before compression.\n",
      "Uploading tarball of [tensorflow_trainer_image] to [gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631460341.83998-8b4d6e59834c4fc19e148543f468e634.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-01-43b0d7048e07/locations/global/builds/105775d5-70aa-43f7-b4d9-89539b369917].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/105775d5-70aa-43f7-b4d9-89539b369917?project=821318692321].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"105775d5-70aa-43f7-b4d9-89539b369917\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631460341.83998-8b4d6e59834c4fc19e148543f468e634.tgz#1631460342050308\n",
      "Copying gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631460341.83998-8b4d6e59834c4fc19e148543f468e634.tgz#1631460342050308...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "04d61753a965: Pulling fs layer\n",
      "4c841a8dc1c2: Pulling fs layer\n",
      "ba19310bc52a: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "187fc16bda8d: Pulling fs layer\n",
      "be760be104b7: Pulling fs layer\n",
      "67ef79a8e023: Pulling fs layer\n",
      "31110891cfb1: Pulling fs layer\n",
      "8c8c23a31282: Pulling fs layer\n",
      "b1bb999ddb16: Pulling fs layer\n",
      "3ce4958fd428: Pulling fs layer\n",
      "77615062896e: Pulling fs layer\n",
      "a3ad053dcd2a: Pulling fs layer\n",
      "60b86c84dfcb: Pulling fs layer\n",
      "bb2debff4a41: Pulling fs layer\n",
      "502011482b4c: Pulling fs layer\n",
      "26c66a4831b0: Pulling fs layer\n",
      "ba19310bc52a: Waiting\n",
      "187fc16bda8d: Waiting\n",
      "be760be104b7: Waiting\n",
      "67ef79a8e023: Waiting\n",
      "31110891cfb1: Waiting\n",
      "8c8c23a31282: Waiting\n",
      "b1bb999ddb16: Waiting\n",
      "3ce4958fd428: Waiting\n",
      "77615062896e: Waiting\n",
      "a3ad053dcd2a: Waiting\n",
      "60b86c84dfcb: Waiting\n",
      "bb2debff4a41: Waiting\n",
      "502011482b4c: Waiting\n",
      "26c66a4831b0: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "04d61753a965: Verifying Checksum\n",
      "04d61753a965: Download complete\n",
      "e4ca327ec0e7: Verifying Checksum\n",
      "e4ca327ec0e7: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "187fc16bda8d: Verifying Checksum\n",
      "187fc16bda8d: Download complete\n",
      "ba19310bc52a: Verifying Checksum\n",
      "ba19310bc52a: Download complete\n",
      "67ef79a8e023: Verifying Checksum\n",
      "67ef79a8e023: Download complete\n",
      "31110891cfb1: Verifying Checksum\n",
      "31110891cfb1: Download complete\n",
      "8c8c23a31282: Verifying Checksum\n",
      "8c8c23a31282: Download complete\n",
      "b1bb999ddb16: Verifying Checksum\n",
      "b1bb999ddb16: Download complete\n",
      "3ce4958fd428: Verifying Checksum\n",
      "3ce4958fd428: Download complete\n",
      "77615062896e: Verifying Checksum\n",
      "77615062896e: Download complete\n",
      "a3ad053dcd2a: Verifying Checksum\n",
      "a3ad053dcd2a: Download complete\n",
      "60b86c84dfcb: Verifying Checksum\n",
      "60b86c84dfcb: Download complete\n",
      "bb2debff4a41: Verifying Checksum\n",
      "bb2debff4a41: Download complete\n",
      "be760be104b7: Verifying Checksum\n",
      "be760be104b7: Download complete\n",
      "26c66a4831b0: Verifying Checksum\n",
      "26c66a4831b0: Download complete\n",
      "4c841a8dc1c2: Verifying Checksum\n",
      "4c841a8dc1c2: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "04d61753a965: Pull complete\n",
      "502011482b4c: Verifying Checksum\n",
      "502011482b4c: Download complete\n",
      "4c841a8dc1c2: Pull complete\n",
      "ba19310bc52a: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "187fc16bda8d: Pull complete\n",
      "be760be104b7: Pull complete\n",
      "67ef79a8e023: Pull complete\n",
      "31110891cfb1: Pull complete\n",
      "8c8c23a31282: Pull complete\n",
      "b1bb999ddb16: Pull complete\n",
      "3ce4958fd428: Pull complete\n",
      "77615062896e: Pull complete\n",
      "a3ad053dcd2a: Pull complete\n",
      "60b86c84dfcb: Pull complete\n",
      "bb2debff4a41: Pull complete\n",
      "502011482b4c: Pull complete\n",
      "26c66a4831b0: Pull complete\n",
      "Digest: sha256:d13107221c97d56ffa891a44beaecaa8438c87102ec7e399f97c77d70c7cd7e4\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> b99046b5f2b5\n",
      "Step 2/5 : RUN pip install -U fire tensorflow==2.1.1\n",
      " ---> Running in c6c838252044\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting tensorflow==2.1.1\n",
      "  Downloading tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.38.1)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (3.16.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (0.37.0)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.16.0)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.19.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.12.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-preprocessing==1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (57.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.4.6)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.1.1)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.5.0)\n",
      "Building wheels for collected packages: gast, fire, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=1ac09fd922bda18101a1c180ed9cf4f11efaf0469070957b4c9a1a5897eab66b\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=c47dfaf49dc09c4b25d1fe0bc18e1a8759fb136bb31bab6bdbd1a02b9720f78e\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=7797ec67ef7095669d6f2fca9618ed9d29fd335e2237889078cee36bdd8e0c5c\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built gast fire termcolor\n",
      "Installing collected packages: cached-property, werkzeug, h5py, absl-py, termcolor, tensorflow-estimator, tensorboard, scipy, opt-einsum, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow, fire\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.1\n",
      "    Uninstalling scipy-1.7.1:\n",
      "      Successfully uninstalled scipy-1.7.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "phik 0.11.2 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "\u001b[0mSuccessfully installed absl-py-0.13.0 astor-0.8.1 cached-property-1.5.2 fire-0.4.0 gast-0.2.2 google-pasta-0.2.0 h5py-3.4.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 opt-einsum-3.3.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.1 tensorflow-estimator-2.1.0 termcolor-1.1.0 werkzeug-2.0.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container c6c838252044\n",
      " ---> 85dd26642a9b\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 6aa599deeb38\n",
      "Removing intermediate container 6aa599deeb38\n",
      " ---> 916f68550eb0\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> c75bdec0aeec\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 99363579e078\n",
      "Removing intermediate container 99363579e078\n",
      " ---> aaaedd6a26e7\n",
      "Successfully built aaaedd6a26e7\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image]\n",
      "9b0459a1aad0: Preparing\n",
      "9c0161797583: Preparing\n",
      "bd1913fe4cd2: Preparing\n",
      "a34559070c4d: Preparing\n",
      "5b1ab540351c: Preparing\n",
      "7e351b6f48b7: Preparing\n",
      "3e4234304903: Preparing\n",
      "fdc0888feca9: Preparing\n",
      "aeae925dc9db: Preparing\n",
      "1bd7873b0779: Preparing\n",
      "376d2b415584: Preparing\n",
      "44f70d274c58: Preparing\n",
      "e7d923b0bae8: Preparing\n",
      "7e2741fe6f4c: Preparing\n",
      "18fbfa48ed76: Preparing\n",
      "51943bd297fe: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "c4594481f563: Preparing\n",
      "97982642ef8a: Preparing\n",
      "251dde02ee49: Preparing\n",
      "6babb56be259: Preparing\n",
      "7e351b6f48b7: Waiting\n",
      "3e4234304903: Waiting\n",
      "fdc0888feca9: Waiting\n",
      "1bd7873b0779: Waiting\n",
      "376d2b415584: Waiting\n",
      "44f70d274c58: Waiting\n",
      "e7d923b0bae8: Waiting\n",
      "7e2741fe6f4c: Waiting\n",
      "18fbfa48ed76: Waiting\n",
      "51943bd297fe: Waiting\n",
      "aeae925dc9db: Waiting\n",
      "251dde02ee49: Waiting\n",
      "6babb56be259: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "c4594481f563: Waiting\n",
      "97982642ef8a: Waiting\n",
      "a34559070c4d: Layer already exists\n",
      "5b1ab540351c: Layer already exists\n",
      "7e351b6f48b7: Layer already exists\n",
      "3e4234304903: Layer already exists\n",
      "fdc0888feca9: Layer already exists\n",
      "aeae925dc9db: Layer already exists\n",
      "1bd7873b0779: Layer already exists\n",
      "376d2b415584: Layer already exists\n",
      "44f70d274c58: Layer already exists\n",
      "e7d923b0bae8: Layer already exists\n",
      "7e2741fe6f4c: Layer already exists\n",
      "18fbfa48ed76: Layer already exists\n",
      "51943bd297fe: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "c4594481f563: Layer already exists\n",
      "97982642ef8a: Layer already exists\n",
      "251dde02ee49: Layer already exists\n",
      "6babb56be259: Layer already exists\n",
      "9b0459a1aad0: Pushed\n",
      "9c0161797583: Pushed\n",
      "bd1913fe4cd2: Pushed\n",
      "latest: digest: sha256:a025e6a796a31fcb7b2cd6481bd8d25fb64a10363e120a31b4bef2615bba2709 size: 4708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                     IMAGES                                                                  STATUS\n",
      "105775d5-70aa-43f7-b4d9-89539b369917  2021-09-12T15:25:42+00:00  6M13S     gs://qwiklabs-gcp-01-43b0d7048e07_cloudbuild/source/1631460341.83998-8b4d6e59834c4fc19e148543f468e634.tgz  gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TF_IMAGE_URI $TF_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f0a35",
   "metadata": {},
   "source": [
    "## Submit an AI Platform training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2dc319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [JOB_20210912_153757] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe JOB_20210912_153757\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs JOB_20210912_153757\n",
      "jobId: JOB_20210912_153757\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "JOB_DIR = \"{}/{}\".format(JOB_DIR_ROOT, JOB_NAME)\n",
    "SCALE_TIER = \"BASIC\"\n",
    "\n",
    "WINDOW_SIZE = 30\n",
    "BATCH_SIZE = 16 \n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "--region=$REGION \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--master-image-uri=$TF_IMAGE_URI \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "-- \\\n",
    "--training_dataset_path=$TRAINING_FILE_PATH \\\n",
    "--window_size=$WINDOW_SIZE \\\n",
    "--epochs=$EPOCHS \\\n",
    "--batch_size=$BATCH_SIZE \\\n",
    "--output_dir=$OUTPUT_DIR \\\n",
    "--lr=$LR \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ecd9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2021-09-12T15:37:59Z'\n",
      "etag: _v0deQoujVg=\n",
      "jobId: JOB_20210912_153757\n",
      "startTime: '2021-09-12T15:42:24Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --training_dataset_path=gs://qwiklabs-gcp-01-43b0d7048e07/data/dataset.csv\n",
      "  - --window_size=30\n",
      "  - --epochs=10\n",
      "  - --batch_size=16\n",
      "  - --output_dir=gs://qwiklabs-gcp-01-43b0d7048e07/models\n",
      "  - --lr=0.001\n",
      "  - \\\n",
      "  jobDir: gs://qwiklabs-gcp-01-43b0d7048e07/jobs/JOB_20210912_153757\n",
      "  masterConfig:\n",
      "    imageUri: gcr.io/qwiklabs-gcp-01-43b0d7048e07/tensorflow_trainer_image:latest\n",
      "  region: us-central1\n",
      "trainingOutput: {}\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/mlengine/jobs/JOB_20210912_153757?project=qwiklabs-gcp-01-43b0d7048e07\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2FJOB_20210912_153757&project=qwiklabs-gcp-01-43b0d7048e07\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c1c87",
   "metadata": {},
   "source": [
    "## HPTUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969be155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tensorflow_trainer_image/hptuning_config.yaml\n",
    "\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 4\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: WINDOW_SIZE\n",
    "      type: DISCRETE\n",
    "      discreteValues: [\n",
    "          200,\n",
    "          500\n",
    "          ]\n",
    "    - parameterName: alpha\n",
    "      type: DOUBLE\n",
    "      minValue:  0.00001\n",
    "      maxValue:  0.001\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m79"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
